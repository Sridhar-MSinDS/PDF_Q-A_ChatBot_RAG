# --------------------------------------
# ‚úÖ STEP 1: Import Required Libraries
# --------------------------------------
import streamlit as st
from PyPDF2 import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_groq import ChatGroq  # üëà For Groq LLM

# --------------------------------------
# ‚úÖ STEP 2: Streamlit App Layout
# --------------------------------------
st.header("üìÑ PDF Chatbot - Ask Questions from Uploaded PDF")

with st.sidebar:
    st.title("üìö Upload Your PDF")
    file = st.file_uploader("Upload a PDF file", type="pdf")

# --------------------------------------
# ‚úÖ STEP 3: Extract Text from PDF
# --------------------------------------
if file is not None:
    pdf_reader = PdfReader(file)
    text = ""
    for page in pdf_reader.pages:
        text += page.extract_text()

    # --------------------------------------
    # ‚úÖ STEP 4: Split Text into Chunks
    # --------------------------------------
    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    chunks = splitter.split_text(text)

    # --------------------------------------
    # ‚úÖ STEP 5: Generate Embeddings using OpenAI
    # --------------------------------------
    OPENAI_API_KEY = "your-openai-api-key"  # üîê Replace with your OpenAI key
    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)

    # --------------------------------------
    # ‚úÖ STEP 6: Store Embeddings in FAISS
    # --------------------------------------
    vector_store = FAISS.from_texts(chunks, embeddings)

    # --------------------------------------
    # ‚úÖ STEP 7: Get User Query & Retrieve Chunks
    # --------------------------------------
    user_query = st.text_input("üí¨ Ask a question from your PDF")

    if user_query:
        matching_chunks = vector_store.similarity_search(user_query)

        # --------------------------------------
        # ‚úÖ STEP 8: Define Groq LLM and Prompt
        # --------------------------------------
        GROQ_API_KEY = "your-groq-api-key"  # üîê Replace with your Groq API key
        llm = ChatGroq(
            api_key=GROQ_API_KEY,
            model="mixtral-8x7b-32768",  # You can also use "llama3-70b-8192"
            temperature=0,
            max_tokens=300
        )

        prompt = ChatPromptTemplate.from_template(
            """
            You are an intelligent and helpful assistant specialized in summarizing and answering questions from study notes.

            Use the provided context extracted from the user's PDF to answer the question accurately and concisely.

            If the answer is not available in the context, say: "I'm sorry, I couldn't find that information in the uploaded PDF."

            ---------------------
            Context:
            {context}

            Question:
            {input}
            ---------------------
            Answer:
            """
        )

        chain = create_stuff_documents_chain(llm, prompt)

        # --------------------------------------
        # ‚úÖ STEP 9: Generate and Display Answer
        # --------------------------------------
        output = chain.invoke({
            "input": user_query,
            "input_documents": matching_chunks
        })

        st.subheader("üß† Answer")
        st.write(output)
